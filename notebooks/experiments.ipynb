{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: D101, D102, D107, E501, FBT003, N806, PD011, PLR2004, S311, T201\n",
    "import math\n",
    "from collections.abc import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from scipy import stats\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eden.utils import change_range, seed_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_func(\n",
    "    func: Callable[[Tensor], Tensor],\n",
    "    seeds: tuple[int, int] | Tensor,\n",
    "    num_steps: int = 100,\n",
    "    step_size: float = 0.1,\n",
    "    noise_size: float | None = None,\n",
    "    range_: tuple[float, float] = (-1, 1),\n",
    ") -> Tensor:\n",
    "    \"\"\"Samples from a distribution using Langevin dynamics.\"\"\"\n",
    "    if isinstance(seeds, tuple):\n",
    "        x = torch.rand(seeds, device=\"cuda\")\n",
    "        x = change_range(x, (0, 1), range_)\n",
    "    else:\n",
    "        x = seeds.clone()\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    noise = torch.empty_like(x, requires_grad=False)\n",
    "    if noise_size is None:\n",
    "        noise_step = math.sqrt(2 * step_size) if step_size > 1 else (2 * step_size) ** 2\n",
    "    else:\n",
    "        noise_step = noise_size\n",
    "\n",
    "    optimizer = optim.SGD([x], lr=step_size)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        y = func(x).sum()\n",
    "        y.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x.clamp_(range_[0], range_[1])\n",
    "\n",
    "            if noise_step > 0:\n",
    "                noise.normal_(0, noise_step)\n",
    "                x.add_(noise)\n",
    "                x.clamp_(range_[0], range_[1])\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "def sample_from_model(\n",
    "    model: nn.Module,\n",
    "    seeds: tuple[int, int] | Tensor,\n",
    "    num_steps: int = 100,\n",
    "    step_size: float = 0.1,\n",
    "    noise_size: float | None = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"Samples from a model using Langevin dynamics.\"\"\"\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    x = sample_from_func(model, seeds, num_steps, step_size, noise_size)\n",
    "\n",
    "    model.requires_grad_(True)\n",
    "    model.train()\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def distance_matrix(\n",
    "    x: Tensor,\n",
    "    y: Tensor,\n",
    "    metric: str = \"euclidean\",\n",
    ") -> Tensor:\n",
    "    \"\"\"Computes the distance matrix between two sets of points.\n",
    "\n",
    "    Args:\n",
    "        x: The first set of points. This must be a tensor of shape (N, D).\n",
    "        y: The second set of points. This must be a tensor of shape (M, D).\n",
    "        metric: The distance metric to use. This can be either \"euclidean\" or \"cosine\".\n",
    "\n",
    "    Returns:\n",
    "        The distance matrix between the two sets of points.\n",
    "        This will be a tensor of shape (N, M).\n",
    "    \"\"\"\n",
    "    match metric:\n",
    "        case \"euclidean\":\n",
    "            return torch.cdist(x, y)\n",
    "        case \"cosine\":\n",
    "            return 1 - F.cosine_similarity(x[:, None, :], y[None, :, :], dim=-1)\n",
    "        case _:\n",
    "            msg = f\"Unknown distance: {metric}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "\n",
    "def estimate_bandwidth(x: Tensor, quantile: float = 0.3) -> float:\n",
    "    \"\"\"Estimates the bandwidth for mean shift clustering.\"\"\"\n",
    "    n_neighbors = int(quantile * x.shape[0])\n",
    "    n_neighbors = max(1, n_neighbors)\n",
    "\n",
    "    dist = distance_matrix(x, x)\n",
    "    # dist.fill_diagonal_(torch.inf)\n",
    "\n",
    "    dist, _ = torch.topk(dist, n_neighbors, largest=False)\n",
    "    dist = dist[:, -1]\n",
    "\n",
    "    return dist.mean().item()\n",
    "\n",
    "\n",
    "def meanshift(\n",
    "    x: Tensor,\n",
    "    num_iters: int = 100,\n",
    "    bandwidth: float | None = None,\n",
    "    centroids: Tensor | None = None,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Clusters the data points using mean shift.\"\"\"\n",
    "    if bandwidth is None:\n",
    "        bandwidth = estimate_bandwidth(x)\n",
    "\n",
    "    if centroids is None:\n",
    "        centroids = x\n",
    "\n",
    "    stop_threshold = 1e-3 * bandwidth\n",
    "    for _ in range(num_iters):\n",
    "        cx_dist = distance_matrix(centroids, x)  # (C, N)\n",
    "        inside = (cx_dist <= bandwidth).float()  # (C, N)\n",
    "\n",
    "        denominator = inside.sum(dim=1, keepdim=True)  # (C, 1)\n",
    "        new_centroids = (inside @ x) / denominator  # (C, D)\n",
    "\n",
    "        mask = (denominator == 0).squeeze_(1)\n",
    "        new_centroids[mask] = centroids[mask]\n",
    "\n",
    "        if torch.norm(new_centroids - centroids) <= stop_threshold:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    cx_dist = distance_matrix(centroids, x)\n",
    "    inside = cx_dist <= bandwidth\n",
    "    count = inside.sum(dim=1)  # (C,)\n",
    "\n",
    "    cc_dist = distance_matrix(centroids, centroids)  # (C, C)\n",
    "    overlap = cc_dist <= bandwidth\n",
    "    overlap.fill_diagonal_(False)  # (C, C)\n",
    "    not_overlap = ~overlap\n",
    "\n",
    "    keep = torch.ones_like(count, dtype=torch.bool)\n",
    "    order = torch.argsort(count, descending=True)\n",
    "    for i in order:\n",
    "        if keep[i]:\n",
    "            keep &= not_overlap[i]\n",
    "\n",
    "    centroids = centroids[keep]\n",
    "\n",
    "    xc_dist = distance_matrix(x, centroids)  # (N, C)\n",
    "    labels = torch.argmin(xc_dist, dim=1)  # (N,)\n",
    "\n",
    "    return centroids, labels\n",
    "\n",
    "\n",
    "def cluster(x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Clusters the data points using mean shift.\n",
    "\n",
    "    Returns:\n",
    "        The cluster centers and the cluster assignments.\n",
    "    \"\"\"\n",
    "    ms = sklearn.cluster.MeanShift(max_iter=100)\n",
    "    ms.fit(x.cpu().numpy())\n",
    "    centers = torch.tensor(ms.cluster_centers_, device=x.device)\n",
    "    labels = torch.tensor(ms.labels_, device=x.device)\n",
    "    return centers, labels\n",
    "\n",
    "\n",
    "def plot_points(x: Tensor, range_: tuple[float, float] | None = None) -> None:\n",
    "    \"\"\"Plots the points.\"\"\"\n",
    "    x = x.cpu()\n",
    "    if x.shape[1] == 1:\n",
    "        plt.hist(x, bins=100)\n",
    "        if range_ is not None:\n",
    "            plt.xlim(*range_)\n",
    "    elif x.shape[1] == 2:\n",
    "        plt.scatter(x[:, 0], x[:, 1])\n",
    "        if range_ is not None:\n",
    "            plt.xlim(*range_)\n",
    "            plt.ylim(*range_)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clusters(x: Tensor, centers: Tensor, labels: Tensor) -> None:\n",
    "    \"\"\"Plots the clusters.\"\"\"\n",
    "    if x.shape[1] != 2:\n",
    "        return\n",
    "\n",
    "    x, centers, labels = x.cpu(), centers.cpu(), labels.cpu()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=labels, cmap=\"tab20\")\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c=\"red\", s=100, marker=\"x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affinity_propagation(\n",
    "    x: Tensor,\n",
    "    damping: float,\n",
    "    max_iters: int,\n",
    "    tol: float = 1e-4,\n",
    ") -> Tensor:\n",
    "    \"\"\"Clusters the data points using affinity propagation.\"\"\"\n",
    "    similarity = F.cosine_similarity(x[:, None, :], x[None, :, :], dim=-1)\n",
    "    median = torch.median(similarity)\n",
    "    similarity.fill_diagonal_(median.item())\n",
    "\n",
    "    responsibility = torch.zeros_like(similarity)\n",
    "    availability = torch.zeros_like(similarity)\n",
    "\n",
    "    converged = False\n",
    "    t = 0\n",
    "    while (not converged) and (t < max_iters):\n",
    "        r_t = _compute_responsibility(similarity, availability)\n",
    "        responsibility = _damper(responsibility, r_t, damping)\n",
    "\n",
    "        a_t = _compute_availability(responsibility)\n",
    "        availability = _damper(availability, a_t, damping)\n",
    "\n",
    "        ch = max(\n",
    "            _chebyshev_distance(r_t, responsibility).max().item(),\n",
    "            _chebyshev_distance(a_t, availability).max().item(),\n",
    "        )\n",
    "        ch = ch / (1 - damping)\n",
    "\n",
    "        converged = ch < tol\n",
    "        t += 1\n",
    "\n",
    "    cost = responsibility + availability\n",
    "    exemplars = torch.nonzero(cost.diagonal() > 0).squeeze(1)\n",
    "    cost = cost[:, exemplars]\n",
    "\n",
    "    labels = torch.argmax(cost, dim=1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helper Functions\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def _compute_responsibility(similarity: Tensor, availability: Tensor) -> Tensor:\n",
    "    p_indices = torch.arange(similarity.shape[0], device=similarity.device)\n",
    "\n",
    "    sum_sa = availability + similarity\n",
    "    first_max, first_max_idx = torch.max(sum_sa, dim=1)\n",
    "    sum_sa[p_indices, first_max_idx] = -torch.inf\n",
    "    second_max, _ = torch.max(sum_sa, dim=1)\n",
    "\n",
    "    r = similarity - first_max.unsqueeze(1)\n",
    "    r[p_indices, first_max_idx] = similarity[p_indices, first_max_idx] - second_max\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def _compute_availability(responsibility: Tensor) -> Tensor:\n",
    "    pos_r = responsibility.clamp_min(0)\n",
    "    pos_r = pos_r.diagonal_scatter(responsibility.diagonal())\n",
    "\n",
    "    a = pos_r.sum(dim=0) - pos_r\n",
    "    a_diagonal = a.diagonal().clone()\n",
    "    a = a.clamp_max(0)\n",
    "    return a.diagonal_scatter(a_diagonal)\n",
    "\n",
    "\n",
    "def _damper(x_p: Tensor, x_t: Tensor, damping: float) -> Tensor:\n",
    "    return x_p * damping + x_t * (1 - damping)\n",
    "\n",
    "\n",
    "def _chebyshev_distance(x: Tensor, y: Tensor) -> Tensor:\n",
    "    return torch.max(torch.abs(x - y), dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the sphere distribution.\"\"\"\n",
    "    return (x**2).sum(dim=1)\n",
    "\n",
    "\n",
    "def rosenbrock(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Rosenbrock function.\"\"\"\n",
    "    first_factor = 100 * (x[:, 1:] - x[:, :-1] ** 2) ** 2\n",
    "    second_factor = (1 - x[:, :-1]) ** 2\n",
    "    return (first_factor + second_factor).sum(dim=1)\n",
    "\n",
    "\n",
    "def rastrigin(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Rastrigin function.\"\"\"\n",
    "    return 10 * x.shape[1] + (x**2 - 10 * torch.cos(2 * math.pi * x)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_from_func(\n",
    "    rastrigin,\n",
    "    (10000, 2),\n",
    "    num_steps=100,\n",
    "    step_size=1e-3,\n",
    "    noise_size=1e-4,\n",
    "    range_=(-5, 5),\n",
    ")\n",
    "\n",
    "# use binary search to find the best bandwidth\n",
    "eps = 1 / 10000\n",
    "low_q = 0 + eps\n",
    "high_q = 1 - eps\n",
    "low_b = estimate_bandwidth(x, low_q)\n",
    "high_b = estimate_bandwidth(x, high_q)\n",
    "\n",
    "while high_q - low_q > eps:\n",
    "    mid_q = (low_q + high_q) / 2\n",
    "    mid_b = estimate_bandwidth(x, mid_q)\n",
    "\n",
    "    if mid_b - low_b > high_b - mid_b:\n",
    "        high_q = mid_q\n",
    "        high_b = mid_b\n",
    "    else:\n",
    "        low_q = mid_q\n",
    "        low_b = mid_b\n",
    "\n",
    "quantile = (low_q + high_q) / 2\n",
    "print(quantile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 2, requires_grad=True)\n",
    "e = sphere(x).sum()\n",
    "grad = torch.autograd.grad(e, x)[0]\n",
    "x, e, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_from_func(\n",
    "    rastrigin,\n",
    "    (10000, 2),\n",
    "    num_steps=200,\n",
    "    step_size=1e-3,\n",
    "    noise_size=0.00,\n",
    "    range_=(-5, 5),\n",
    ")\n",
    "centroids, labels = meanshift(x, bandwidth=estimate_bandwidth(x, 0.01))\n",
    "plot_clusters(x, centroids, labels)\n",
    "min_f = rastrigin(x).min().item()\n",
    "min_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid = centroids[0]\n",
    "points = x[labels == 0]\n",
    "\n",
    "# find points whose distance from the centroid is less than the median distance\n",
    "dist = torch.cdist(points, centroid.unsqueeze(0)).squeeze()\n",
    "mask = dist <= dist.median()\n",
    "mask = mask.cpu()\n",
    "points = points.cpu()\n",
    "centroid = centroid.cpu()\n",
    "\n",
    "plt.plot(points[mask][:, 0], points[mask][:, 1], \"o\", color=\"blue\")\n",
    "plt.plot(points[~mask][:, 0], points[~mask][:, 1], \"o\", color=\"red\")\n",
    "plt.plot(centroid[0], centroid[1], \"x\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGE = (-5, 5)\n",
    "NUM_POINTS = 10000\n",
    "GENOME_SIZE = 100\n",
    "BASE_LR = 1e-3\n",
    "BASE_NOISE = 1e-4\n",
    "QUANTILE = 0.2\n",
    "\n",
    "energy = rastrigin\n",
    "\n",
    "\n",
    "def sample(x: Tensor, step_size: Tensor, noise_size: Tensor | None = None) -> Tensor:\n",
    "    \"\"\"Samples from a distribution using Langevin dynamics.\"\"\"\n",
    "    x = x.clone()\n",
    "    x.requires_grad_(True)\n",
    "    if noise_size is None:\n",
    "        noise_size = step_size**2\n",
    "\n",
    "    noise = torch.empty_like(x, requires_grad=False)\n",
    "\n",
    "    for _ in range(50):\n",
    "        y = energy(x).sum()\n",
    "        grad = torch.autograd.grad(y, x)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x.sub_(step_size * grad)\n",
    "            x.clamp_(*RANGE)\n",
    "\n",
    "            noise.normal_(0, 1)\n",
    "            x.add_(noise.mul_(noise_size))\n",
    "            x.clamp_(*RANGE)\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "x = torch.rand((NUM_POINTS, GENOME_SIZE), device=\"cuda\").mul_(2).sub_(1)\n",
    "lr = torch.full_like(x, BASE_LR)\n",
    "noise_size = torch.full_like(x, BASE_NOISE)\n",
    "prev_clusters = []\n",
    "\n",
    "for _ in range(20):\n",
    "    x = sample(x, lr, noise_size)\n",
    "    f = energy(x)\n",
    "\n",
    "    min_f, mean_f, max_f = f.min().item(), f.mean().item(), f.max().item()\n",
    "    print(f\"Min: {min_f:.2E}, Mean: {mean_f:.2E}, Max: {max_f:.2E}\")\n",
    "    if min_f < 1e-20:\n",
    "        print(\"Minimum reached\")\n",
    "        break\n",
    "\n",
    "    bandwidth = estimate_bandwidth(x, quantile=QUANTILE)\n",
    "    centroids, labels = meanshift(x, num_iters=100, bandwidth=bandwidth)\n",
    "    plot_clusters(x, centroids, labels)\n",
    "\n",
    "    new_clusters: list[tuple[Tensor, float]] = []\n",
    "    for j in range(centroids.shape[0]):\n",
    "        points = x[labels == j]\n",
    "        fitness = float(energy(points).mean())\n",
    "        new_clusters.append((points, fitness))\n",
    "\n",
    "    f_denominator = sum(1 / f for _, f in new_clusters)\n",
    "\n",
    "    new_points = []\n",
    "    new_noise_sizes = []\n",
    "    for i, (c_points, c_fitness) in enumerate(new_clusters):\n",
    "        N = math.ceil(NUM_POINTS * (1 / c_fitness) / f_denominator)\n",
    "        if N <= GENOME_SIZE:\n",
    "            continue\n",
    "\n",
    "        mean, cov = torch.mean(c_points, dim=0), torch.cov(c_points.T)\n",
    "        # make sure the covariance matrix is positive definite\n",
    "        cov = cov + torch.eye(GENOME_SIZE, device=cov.device) * 1e-6\n",
    "        mn = torch.distributions.MultivariateNormal(mean, cov)\n",
    "        p = mn.rsample((N,))\n",
    "        new_points.append(p)\n",
    "\n",
    "        best_match, best_icd = None, torch.inf\n",
    "        for j in range(len(prev_clusters)):\n",
    "            icd = distance_matrix(c_points, prev_clusters[j][0])\n",
    "            icd = icd.min(dim=1).values.mean().item()\n",
    "            # print(f\"ICD: {icd:.2f}, bandwidth: {bandwidth:.2f}\")\n",
    "            if icd < best_icd and icd < bandwidth:\n",
    "                best_match, best_icd = j, icd\n",
    "\n",
    "        if best_match is not None:\n",
    "            # print(f\"Matched with cluster {best_match}\")\n",
    "            _, prev_fitness = prev_clusters[best_match]\n",
    "            factor = math.sqrt(c_fitness / prev_fitness)\n",
    "            new_clusters[i] = (c_points, min(c_fitness, prev_fitness))\n",
    "            # print(f\"Factor: {factor:.2f}\")\n",
    "        else:\n",
    "            factor = 1.0\n",
    "\n",
    "        extension = c_points.max(dim=0).values - c_points.min(dim=0).values / 2\n",
    "        c_noise_size = torch.full((N, GENOME_SIZE), BASE_NOISE, device=\"cuda\")\n",
    "        c_noise_size = c_noise_size * extension * factor\n",
    "        new_noise_sizes.append(c_noise_size)\n",
    "\n",
    "    prev_clusters = new_clusters\n",
    "    x = torch.cat(new_points, dim=0)\n",
    "    x.clamp_(*RANGE)\n",
    "    lr = torch.full_like(x, BASE_LR)\n",
    "    noise_size = torch.cat(new_noise_sizes, dim=0)\n",
    "\n",
    "    plot_points(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x: Tensor, step_size: Tensor, noise_size: Tensor | None = None) -> Tensor:\n",
    "    \"\"\"Samples from a distribution using Langevin dynamics.\"\"\"\n",
    "    x = x.clone()\n",
    "    x.requires_grad_(True)\n",
    "    if noise_size is None:\n",
    "        noise_size = step_size**2\n",
    "\n",
    "    noise = torch.empty_like(x, requires_grad=False)\n",
    "\n",
    "    for _ in range(50):\n",
    "        y = sphere(x).sum()\n",
    "        grad = torch.autograd.grad(y, x)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x.sub_(step_size * grad)\n",
    "            x.clamp_(-1, 1)\n",
    "\n",
    "            noise.normal_(0, 1)\n",
    "            x.add_(noise.mul_(noise_size))\n",
    "            x.clamp_(-1, 1)\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "NUM_POINTS = 10000\n",
    "GENOME_SIZE = 10\n",
    "x = torch.rand((NUM_POINTS, GENOME_SIZE), device=\"cuda\").mul_(2).sub_(1)\n",
    "lr = torch.full_like(x, 0.1)\n",
    "noise_size = torch.full_like(x, 0.01)\n",
    "prev_clusters = []\n",
    "\n",
    "for _ in range(20):\n",
    "    x = sample(x, lr, noise_size)\n",
    "    f = sphere(x)\n",
    "\n",
    "    min_f, mean_f, max_f = f.min().item(), f.mean().item(), f.max().item()\n",
    "    print(f\"Min: {min_f:.2E}, Mean: {mean_f:.2E}, Max: {max_f:.2E}\")\n",
    "    if min_f < 1e-20:\n",
    "        print(\"Minimum reached\")\n",
    "        break\n",
    "\n",
    "    bandwidth = estimate_bandwidth(x)\n",
    "    centroids, labels = meanshift(x, num_iters=100, bandwidth=bandwidth)\n",
    "\n",
    "    # remove from each cluster the points that are far from the centroid\n",
    "    for j in range(centroids.shape[0]):\n",
    "        membership = labels == j\n",
    "        points = x[membership]\n",
    "        centroid = points.mean(dim=0)\n",
    "\n",
    "        dist = distance_matrix(points, centroid[None]).squeeze_(1)\n",
    "        mask = dist <= dist.median()\n",
    "        tmp_labels = torch.full(mask.shape, -1, dtype=labels.dtype, device=labels.device)  # fmt: skip\n",
    "        tmp_labels[mask] = j\n",
    "\n",
    "        labels[membership] = tmp_labels\n",
    "\n",
    "    not_membership = labels == -1\n",
    "    not_membership_count = int(not_membership.sum())\n",
    "    x[not_membership] = torch.rand((not_membership_count, GENOME_SIZE), device=\"cuda\").mul_(2).sub_(1)  # fmt: skip\n",
    "\n",
    "    noise_size = torch.full_like(x, 0.01)\n",
    "    for j in range(centroids.shape[0]):\n",
    "        membership = labels == j\n",
    "        points = x[membership]\n",
    "\n",
    "        if points.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        extension = points.max(dim=0).values - points.min(dim=0).values / 2\n",
    "        noise_size[membership] = extension * 0.01\n",
    "\n",
    "    lr = torch.full_like(x, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basis_functions(x: Tensor, knots: Tensor, k: int) -> Tensor:\n",
    "    \"\"\"Computes the b-spline basis functions for the given input data.\n",
    "\n",
    "    Args:\n",
    "        x: The points at which to evaluate the basis functions. This should be a\n",
    "            tensor of shape `(n_splines, n_points)`.\n",
    "        knots: The knot vector. This should be a tensor of shape `(n_splines, n_knots)`.\n",
    "        k: The degree of the spline.\n",
    "\n",
    "    Returns:\n",
    "        The basis functions evaluated at the given points. This is a tensor of\n",
    "        shape `(n_splines, n_basis_functions, n_points)`. The number of basis functions\n",
    "        is equal to `n_knots - k - 1`.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        x = x.unsqueeze(1)  # (n_splines, 1, n_points)\n",
    "        knots = knots.unsqueeze(2)  # (n_splines, n_knots, 1)\n",
    "        bf_k = (x >= knots[:, :-1]) & (x < knots[:, 1:])\n",
    "        bf_k = bf_k.to(x.dtype)\n",
    "    else:\n",
    "        bf_k_minus_1 = compute_basis_functions(x, knots, k - 1)\n",
    "        x = x.unsqueeze(1)  # (n_splines, 1, n_points)\n",
    "        knots = knots.unsqueeze(2)  # (n_splines, n_knots, 1)\n",
    "\n",
    "        first = (x - knots[:, : -(k + 1)]) / (knots[:, k:-1] - knots[:, : -(k + 1)])\n",
    "        first = first * bf_k_minus_1[:, :-1]\n",
    "\n",
    "        second = (knots[:, k + 1 :] - x) / (knots[:, k + 1 :] - knots[:, 1:(-k)])\n",
    "        second = second * bf_k_minus_1[:, 1:]\n",
    "\n",
    "        bf_k = first + second\n",
    "\n",
    "    return bf_k\n",
    "\n",
    "\n",
    "def compute_b_spline(\n",
    "    x: Tensor,\n",
    "    knots: Tensor,\n",
    "    coeffs: Tensor,\n",
    "    k: int,\n",
    ") -> Tensor:\n",
    "    \"\"\"Computes the b-spline function for the given input data.\n",
    "\n",
    "    Args:\n",
    "        x: The points at which to evaluate the b-spline. This should be a tensor\n",
    "            of shape `(n_splines, n_points)`.\n",
    "        knots: The knot vector. This should be a tensor of shape `(n_splines, n_knots)`.\n",
    "        coeffs: The coefficients of the b-spline. This should be a tensor of shape\n",
    "            `(n_splines, n_coeffs)`.\n",
    "        k: The degree of the spline.\n",
    "\n",
    "    Returns:\n",
    "        The b-spline evaluated at the given points. This is a tensor of shape\n",
    "        `(n_splines, n_points)`.\n",
    "    \"\"\"\n",
    "    # (n_splines, n_coeffs, n_points)\n",
    "    basis_functions = compute_basis_functions(x, knots, k)\n",
    "    # (n_splines, n_points)\n",
    "    b_spline = torch.einsum(\"ij,ijk->ik\", coeffs, basis_functions)\n",
    "\n",
    "    return b_spline\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class SplineModel(nn.Module):\n",
    "    \"\"\"A model taht uses B-spline encoding to learn an energy function.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        degree: int = 3,\n",
    "        grid_size: int = 5,\n",
    "        embed_dim: int = 64,\n",
    "        pooling: str = \"mean\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.degree = 3\n",
    "        num_internal_knots = grid_size + 1\n",
    "        num_knots = num_internal_knots + 2 * degree\n",
    "        knots = torch.linspace(-1, 1, num_internal_knots)\n",
    "        diff = knots[1] - knots[0]\n",
    "        knots = torch.cat([\n",
    "            knots[0] - diff * torch.arange(degree, 0, -1),\n",
    "            knots,\n",
    "            knots[-1] + diff * torch.arange(1, degree + 1),\n",
    "        ])\n",
    "        self.register_buffer(\"knots\", knots)\n",
    "        self.knots: Tensor\n",
    "\n",
    "        num_control_points = num_knots - degree - 1\n",
    "        # n_splines = embed_dim\n",
    "        cp = torch.empty(embed_dim, num_control_points)\n",
    "        self.cp = nn.Parameter(cp, requires_grad=True)\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return f\"spline-{self.pooling}\"\n",
    "\n",
    "    @torch.no_grad()  # type: ignore\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.normal_(self.cp, mean=0.0, std=0.1)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        cp = self.cp  # (embed_dim, num_control_points)\n",
    "\n",
    "        N, D = x.shape\n",
    "        x = x.flatten()  # (N * D,)\n",
    "        x = x.unsqueeze(0).expand(cp.size(0), -1)  # (embed_dim, N * D)\n",
    "\n",
    "        knots = self.knots.unsqueeze(0).expand(cp.size(0), -1)  # (embed_dim, num_knots)\n",
    "\n",
    "        embeds = compute_b_spline(x, knots, cp, self.degree)  # (embed_dim, N * D)\n",
    "        embeds = embeds.view(cp.size(0), N, D)  # (embed_dim, N, D)\n",
    "        embeds = embeds.permute(1, 2, 0)  # (N, D, embed_dim)\n",
    "\n",
    "        match self.pooling:\n",
    "            case \"max\":\n",
    "                embeds = embeds.max(dim=1).values\n",
    "            case \"mean\":\n",
    "                embeds = embeds.mean(dim=1)\n",
    "            case \"sum\":\n",
    "                embeds = embeds.sum(dim=1)\n",
    "            case _:\n",
    "                msg = f\"Unknown pooling method: {self.pooling}\"\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        energies = self.mlp(embeds)\n",
    "        return energies.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_new_data(\n",
    "#     model: nn.Module,\n",
    "#     seeds: tuple[int, int] | Tensor,\n",
    "#     num_new_points: int,\n",
    "#     random_ratio: float = 0.1,\n",
    "# ) -> tuple[Tensor, Tensor]:\n",
    "#     \"\"\"Generates new data points using the model.\"\"\"\n",
    "#     # sample from the model an high number of points\n",
    "#     x = sample_from_model(model, seeds, num_steps=50, step_size=0.1, noise_size=0.01)\n",
    "#     centroids, labels = meanshift(x)\n",
    "\n",
    "#     _, counts = torch.unique(labels, return_counts=True)\n",
    "#     fractions: Tensor = counts.float() / labels.size(0)\n",
    "\n",
    "#     num_random_points = round(num_new_points * random_ratio)\n",
    "#     num_cluster_points = num_new_points - num_random_points\n",
    "#     num_points_per_cluster = (fractions * num_cluster_points).round().int()\n",
    "#     num_points_per_cluster.clamp_(min=1)\n",
    "\n",
    "#     new_points = []\n",
    "#     for i, num_points in enumerate(num_points_per_cluster.tolist()):\n",
    "#         cluster_points = x[labels == i]\n",
    "#         extension = cluster_points.max(dim=0).values - cluster_points.min(dim=0).values\n",
    "\n",
    "#         p = torch.rand(num_points, x.size(1), device=x.device).mul_(2).sub_(1)\n",
    "#         p.mul_(extension).div_(2).add_(centroids[i])\n",
    "\n",
    "#         new_points.append(p)\n",
    "\n",
    "#     # add random points\n",
    "#     p = torch.rand(num_random_points, x.size(1), device=x.device).mul_(2).sub_(1)\n",
    "#     new_points.append(p)\n",
    "\n",
    "#     new_points = torch.cat(new_points)\n",
    "#     return new_points, x\n",
    "\n",
    "\n",
    "def generate_new_data(\n",
    "    model: nn.Module,\n",
    "    seeds: tuple[int, int] | Tensor,\n",
    "    num_new_points: int,\n",
    "    random_ratio: float = 0.1,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Generates new data points using the model.\"\"\"\n",
    "    # sample from the model an high number of points\n",
    "    x = sample_from_model(model, seeds, num_steps=50, step_size=0.1, noise_size=0.01)\n",
    "    _, labels = meanshift(x)\n",
    "\n",
    "    _, counts = torch.unique(labels, return_counts=True)\n",
    "    fractions: Tensor = counts.float() / labels.size(0)\n",
    "\n",
    "    num_random_points = round(num_new_points * random_ratio)\n",
    "    num_cluster_points = num_new_points - num_random_points\n",
    "    num_points_per_cluster = (fractions * num_cluster_points).round().int()\n",
    "    num_points_per_cluster.clamp_(min=1)\n",
    "\n",
    "    new_points = []\n",
    "    for i, num_points in enumerate(num_points_per_cluster.tolist()):\n",
    "        cluster_points = x[labels == i]\n",
    "        mean, cov = stats.multivariate_normal.fit(cluster_points.cpu().numpy())\n",
    "        dist = stats.multivariate_normal(mean=mean, cov=cov)\n",
    "        p = torch.tensor(dist.rvs(num_points), device=x.device, dtype=x.dtype)\n",
    "        if num_points == 1:\n",
    "            p = p.unsqueeze(0)\n",
    "        new_points.append(p)\n",
    "\n",
    "    # add random points\n",
    "    p = torch.rand(num_random_points, x.size(1), device=x.device).mul_(2).sub_(1)\n",
    "    new_points.append(p)\n",
    "\n",
    "    new_points = torch.cat(new_points)\n",
    "    return new_points, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(3407)\n",
    "GENOME_SIZE = 2\n",
    "FITNESS_BOUND = 5**2 * GENOME_SIZE\n",
    "DATASET_SIZE = 1_000\n",
    "BATCH_DIM = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "model = SplineModel(pooling=\"mean\").cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=NUM_EPOCHS, gamma=0.1)\n",
    "\n",
    "# populate buffer with initial samples\n",
    "x = torch.rand(DATASET_SIZE, GENOME_SIZE).mul_(2).sub_(1)\n",
    "y = (change_range(x, (-1, 1), (-5, 5)) ** 2).sum(dim=-1) / FITNESS_BOUND\n",
    "\n",
    "dataset = data.TensorDataset(x, y)\n",
    "loader = data.DataLoader(dataset, batch_size=BATCH_DIM, shuffle=True)\n",
    "\n",
    "for _ in tqdm(range(NUM_EPOCHS), desc=\"Training\", total=NUM_EPOCHS):\n",
    "    for x, y in loader:\n",
    "        x, y = x.cuda(), y.cuda()  # noqa: PLW2901\n",
    "        optimizer.zero_grad()\n",
    "        e = model(x)\n",
    "        loss = F.mse_loss(e, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_from_model(model, (1000, GENOME_SIZE), num_steps=50, step_size=0.1, noise_size=0.01)  # fmt: skip\n",
    "clusters, labels = cluster(x)\n",
    "plot_clusters(x, clusters, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = generate_new_data(model, 100, random_ratio=0.0)\n",
    "plot_points(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(3407)\n",
    "GENOME_SIZE = 2\n",
    "FITNESS_BOUND = 5**2 * GENOME_SIZE\n",
    "NUM_EXTERNAL_EPOCHS = 20\n",
    "NUM_NEW_POINTS = 100\n",
    "NUM_INITIAL_POINTS = 1_000\n",
    "NUM_INTERNAL_EPOCHS = 10\n",
    "BATCH_DIM = 128\n",
    "\n",
    "model = SplineModel(pooling=\"mean\").cuda()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "# create initial dataset\n",
    "x = torch.rand(NUM_INITIAL_POINTS, GENOME_SIZE).mul_(2).sub_(1)\n",
    "y = (change_range(x, (-1, 1), (-5, 5)) ** 2).sum(dim=-1) / FITNESS_BOUND\n",
    "dataset = data.TensorDataset(x, y)\n",
    "loader = data.DataLoader(dataset, batch_size=BATCH_DIM, shuffle=True, pin_memory=True)\n",
    "\n",
    "prev_sampled_points = torch.rand(1000, GENOME_SIZE, device=\"cuda\").mul_(2).sub_(1)\n",
    "generated, sampled = [], []\n",
    "\n",
    "for _ in tqdm(range(NUM_EXTERNAL_EPOCHS), desc=\"Training\", total=NUM_EXTERNAL_EPOCHS):\n",
    "    # train for INTERNAL_EPOCHS on the current dataset\n",
    "    for _ in tqdm(range(NUM_INTERNAL_EPOCHS), desc=\"Internal Training\", total=NUM_INTERNAL_EPOCHS):  # fmt: skip\n",
    "        optimizer.zero_grad()\n",
    "        for x, y in loader:\n",
    "            x, y = x.cuda(), y.cuda()  # noqa: PLW2901\n",
    "            e = model(x)\n",
    "            loss = F.mse_loss(e, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # generate new data points\n",
    "    x, prev_sampled_points = generate_new_data(model, (1000, GENOME_SIZE), NUM_NEW_POINTS, random_ratio=0.2)  # fmt: skip\n",
    "    # new_x, prev_sampled_points = generate_new_data(model, prev_sampled_points, NUM_NEW_POINTS, random_ratio=0.2)  # fmt: skip\n",
    "    generated.append(x)\n",
    "    sampled.append(prev_sampled_points)\n",
    "    new_y = (change_range(x, (-1, 1), (-5, 5)) ** 2).sum(dim=-1) / FITNESS_BOUND\n",
    "    old_x = dataset.tensors[0]\n",
    "    old_y = dataset.tensors[1]\n",
    "\n",
    "    # combine old and new data points\n",
    "    x = torch.cat([old_x, x.cpu()])\n",
    "    y = torch.cat([old_y, new_y.cpu()])\n",
    "    dataset = data.TensorDataset(x, y)\n",
    "    loader = data.DataLoader(dataset, batch_size=BATCH_DIM, shuffle=True, pin_memory=True)  # fmt: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sampled:\n",
    "    plot_points(x, range_=(-0.01, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_from_model(model, (10000, GENOME_SIZE), num_steps=50, step_size=0.1, noise_size=0.01)  # fmt: skip\n",
    "# y = (change_range(x, (-1, 1), (-5, 5)) ** 2).sum(dim=-1)\n",
    "y = model(x)\n",
    "\n",
    "_, min_idx = y.min(dim=0)\n",
    "min_x = x[min_idx]\n",
    "min_x, y[min_idx], (change_range(min_x, (-1, 1), (-5, 5)) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = min_x.clone()\n",
    "x.requires_grad = True\n",
    "\n",
    "optimizer = optim.SGD([x], lr=0.001)\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "\n",
    "for _ in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    e = model(x.unsqueeze(0))\n",
    "    e.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.train()\n",
    "model.requires_grad_(True)\n",
    "\n",
    "x = x.detach()\n",
    "x = change_range(x, (-1, 1), (-5, 5))\n",
    "x, (x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splot the 2d landscape\n",
    "# let's  not focus on the whole space, but on the [-1, 1] x [-1, 1] subspace\n",
    "# to verify whether the model has correctly learned the landscape for the points\n",
    "# with better fitness\n",
    "NUM_SAMPLES = 200\n",
    "with torch.no_grad():\n",
    "    low, high = -0.001, 0.001\n",
    "    elev, azim = None, None\n",
    "    x = torch.linspace(low, high, NUM_SAMPLES)\n",
    "    y = torch.linspace(low, high, NUM_SAMPLES)\n",
    "    points = torch.cartesian_prod(x, y)\n",
    "    X = points[:, 0].reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "    Y = points[:, 1].reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "    # X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "    Z = X**2 + Y**2\n",
    "\n",
    "    n_plots = len(models) + 1\n",
    "    fig = plt.figure(figsize=(n_plots * 5, 5))\n",
    "    axes = fig.subplots(1, n_plots, subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "    axes[0].plot_surface(X.cpu(), Y.cpu(), Z.cpu(), cmap=\"viridis\")\n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_zlabel(\"fitness\")\n",
    "    axes[0].view_init(elev=elev, azim=azim)\n",
    "\n",
    "    solutions = points.cuda()\n",
    "    solutions = change_range(solutions, (-5, 5), (-1, 1))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        f_hat = model(solutions) * FITNESS_BOUND\n",
    "        f_hat = f_hat.reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "\n",
    "        axes[j + 1].plot_surface(X.cpu(), Y.cpu(), f_hat.cpu(), cmap=\"viridis\")\n",
    "        axes[j + 1].set_title(model.name)\n",
    "        axes[j + 1].set_xlabel(\"x\")\n",
    "        axes[j + 1].set_ylabel(\"y\")\n",
    "        axes[j + 1].set_zlabel(\"fitness\")\n",
    "\n",
    "        # change rotation\n",
    "        axes[j + 1].view_init(elev=elev, azim=azim)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
