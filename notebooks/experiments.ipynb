{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "\n",
    "All the necessary libraries are imported in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: D101, D102, D107, E501, FBT003, N806, PD011, PLR0913, PLR2004, S311, T201\n",
    "import math\n",
    "import random\n",
    "from collections.abc import Callable\n",
    "from typing import Literal\n",
    "\n",
    "import entmax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import torch\n",
    "import torch.nn.functional as F  # noqa: N812\n",
    "from scipy import stats\n",
    "from torch import Tensor, nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Seeds all random number generators.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)  # noqa: NPY002\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def change_range(\n",
    "    x: Tensor,\n",
    "    old_range: tuple[float, float],\n",
    "    new_range: tuple[float, float],\n",
    ") -> Tensor:\n",
    "    \"\"\"Changes the range of each element in the tensor.\n",
    "\n",
    "    !!! note\n",
    "        If the old range is equal to the new range, then the tensor is returned as is.\n",
    "\n",
    "    Args:\n",
    "        x: The tensor to change the range of.\n",
    "        old_range: The old range of the tensor.\n",
    "        new_range: The new range of the tensor.\n",
    "\n",
    "    Returns:\n",
    "        The tensor with the new range.\n",
    "    \"\"\"\n",
    "    if old_range == new_range:\n",
    "        return x\n",
    "\n",
    "    old_min, old_max = old_range\n",
    "    new_min, new_max = new_range\n",
    "\n",
    "    return ((x - old_min) / (old_max - old_min)) * (new_max - new_min) + new_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_func(\n",
    "    func: Callable[[Tensor], Tensor],\n",
    "    seeds: tuple[int, int] | Tensor,\n",
    "    num_steps: int = 100,\n",
    "    step_size: float = 0.1,\n",
    "    noise_size: float | None = None,\n",
    "    range_: tuple[float, float] = (-1, 1),\n",
    ") -> Tensor:\n",
    "    \"\"\"Samples from a distribution using Langevin dynamics.\"\"\"\n",
    "    if isinstance(seeds, tuple):\n",
    "        x = torch.rand(seeds, device=\"cuda\")\n",
    "        x = change_range(x, (0, 1), range_)\n",
    "    else:\n",
    "        x = seeds.clone()\n",
    "\n",
    "    x.requires_grad_(True)\n",
    "\n",
    "    noise = torch.empty_like(x, requires_grad=False)\n",
    "    if noise_size is None:\n",
    "        noise_step = math.sqrt(2 * step_size) if step_size > 1 else (2 * step_size) ** 2\n",
    "    else:\n",
    "        noise_step = noise_size\n",
    "\n",
    "    optimizer = optim.SGD([x], lr=step_size)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        y = func(x).sum()\n",
    "        y.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x.clamp_(range_[0], range_[1])\n",
    "\n",
    "            if noise_step > 0:\n",
    "                noise.normal_(0, noise_step)\n",
    "                x.add_(noise)\n",
    "                x.clamp_(range_[0], range_[1])\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "def sample_from_model(\n",
    "    model: nn.Module,\n",
    "    seeds: tuple[int, int] | Tensor,\n",
    "    num_steps: int = 100,\n",
    "    step_size: float = 0.1,\n",
    "    noise_size: float | None = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"Samples from a model using Langevin dynamics.\"\"\"\n",
    "    model.requires_grad_(False)\n",
    "    model.eval()\n",
    "\n",
    "    x = sample_from_func(model, seeds, num_steps, step_size, noise_size)\n",
    "\n",
    "    model.requires_grad_(True)\n",
    "    model.train()\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def distance_matrix(\n",
    "    x: Tensor,\n",
    "    y: Tensor,\n",
    "    metric: str = \"euclidean\",\n",
    ") -> Tensor:\n",
    "    \"\"\"Computes the distance matrix between two sets of points.\n",
    "\n",
    "    Args:\n",
    "        x: The first set of points. This must be a tensor of shape (N, D).\n",
    "        y: The second set of points. This must be a tensor of shape (M, D).\n",
    "        metric: The distance metric to use. This can be either \"euclidean\" or \"cosine\".\n",
    "\n",
    "    Returns:\n",
    "        The distance matrix between the two sets of points.\n",
    "        This will be a tensor of shape (N, M).\n",
    "    \"\"\"\n",
    "    match metric:\n",
    "        case \"euclidean\":\n",
    "            return torch.cdist(x, y)\n",
    "        case \"cosine\":\n",
    "            return 1 - F.cosine_similarity(x[:, None, :], y[None, :, :], dim=-1)\n",
    "        case _:\n",
    "            msg = f\"Unknown distance: {metric}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "\n",
    "def estimate_bandwidth(x: Tensor, quantile: float = 0.3) -> float:\n",
    "    \"\"\"Estimates the bandwidth for mean shift clustering.\"\"\"\n",
    "    n_neighbors = int(quantile * x.shape[0])\n",
    "    n_neighbors = max(1, n_neighbors)\n",
    "\n",
    "    dist = distance_matrix(x, x)\n",
    "    # dist.fill_diagonal_(torch.inf)\n",
    "\n",
    "    dist, _ = torch.topk(dist, n_neighbors, largest=False)\n",
    "    dist = dist[:, -1]\n",
    "\n",
    "    return dist.mean().item()\n",
    "\n",
    "\n",
    "def meanshift(\n",
    "    x: Tensor,\n",
    "    num_iters: int = 100,\n",
    "    bandwidth: float | None = None,\n",
    "    centroids: Tensor | None = None,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Clusters the data points using mean shift.\"\"\"\n",
    "    if bandwidth is None:\n",
    "        bandwidth = estimate_bandwidth(x)\n",
    "\n",
    "    if centroids is None:\n",
    "        centroids = x\n",
    "\n",
    "    stop_threshold = 1e-3 * bandwidth\n",
    "    for _ in range(num_iters):\n",
    "        cx_dist = distance_matrix(centroids, x)  # (C, N)\n",
    "        inside = (cx_dist <= bandwidth).float()  # (C, N)\n",
    "\n",
    "        denominator = inside.sum(dim=1, keepdim=True)  # (C, 1)\n",
    "        new_centroids = (inside @ x) / denominator  # (C, D)\n",
    "\n",
    "        mask = (denominator == 0).squeeze_(1)\n",
    "        new_centroids[mask] = centroids[mask]\n",
    "\n",
    "        if torch.norm(new_centroids - centroids) <= stop_threshold:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    cx_dist = distance_matrix(centroids, x)\n",
    "    inside = cx_dist <= bandwidth\n",
    "    count = inside.sum(dim=1)  # (C,)\n",
    "\n",
    "    cc_dist = distance_matrix(centroids, centroids)  # (C, C)\n",
    "    overlap = cc_dist <= bandwidth\n",
    "    overlap.fill_diagonal_(False)  # (C, C)\n",
    "    not_overlap = ~overlap\n",
    "\n",
    "    keep = torch.ones_like(count, dtype=torch.bool)\n",
    "    order = torch.argsort(count, descending=True)\n",
    "    for i in order:\n",
    "        if keep[i]:\n",
    "            keep &= not_overlap[i]\n",
    "\n",
    "    centroids = centroids[keep]\n",
    "\n",
    "    xc_dist = distance_matrix(x, centroids)  # (N, C)\n",
    "    labels = torch.argmin(xc_dist, dim=1)  # (N,)\n",
    "\n",
    "    return centroids, labels\n",
    "\n",
    "\n",
    "def cluster(x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Clusters the data points using mean shift.\n",
    "\n",
    "    Returns:\n",
    "        The cluster centers and the cluster assignments.\n",
    "    \"\"\"\n",
    "    ms = sklearn.cluster.MeanShift(max_iter=100)\n",
    "    ms.fit(x.cpu().numpy())\n",
    "    centers = torch.tensor(ms.cluster_centers_, device=x.device)\n",
    "    labels = torch.tensor(ms.labels_, device=x.device)\n",
    "    return centers, labels\n",
    "\n",
    "\n",
    "def plot_points(x: Tensor, range_: tuple[float, float] | None = None) -> None:\n",
    "    \"\"\"Plots the points.\"\"\"\n",
    "    x = x.cpu()\n",
    "    if x.shape[1] == 1:\n",
    "        plt.hist(x, bins=100)\n",
    "        if range_ is not None:\n",
    "            plt.xlim(*range_)\n",
    "    elif x.shape[1] == 2:\n",
    "        plt.scatter(x[:, 0], x[:, 1])\n",
    "        if range_ is not None:\n",
    "            plt.xlim(*range_)\n",
    "            plt.ylim(*range_)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clusters(x: Tensor, centers: Tensor, labels: Tensor) -> None:\n",
    "    \"\"\"Plots the clusters.\"\"\"\n",
    "    if x.shape[1] != 2:\n",
    "        return\n",
    "\n",
    "    x, centers, labels = x.cpu(), centers.cpu(), labels.cpu()\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=labels, cmap=\"tab20\")\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c=\"red\", s=100, marker=\"x\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sphere(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the sphere distribution.\"\"\"\n",
    "    return (x**2).sum(dim=1)\n",
    "\n",
    "\n",
    "def rosenbrock(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Rosenbrock function.\"\"\"\n",
    "    first_factor = 100 * (x[:, 1:] - x[:, :-1] ** 2) ** 2\n",
    "    second_factor = (1 - x[:, :-1]) ** 2\n",
    "    return (first_factor + second_factor).sum(dim=1)\n",
    "\n",
    "\n",
    "def rastrigin(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Rastrigin function.\"\"\"\n",
    "    return 10 * x.shape[1] + (x**2 - 10 * torch.cos(2 * math.pi * x)).sum(dim=1)\n",
    "\n",
    "\n",
    "def ackley(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Ackley function.\"\"\"\n",
    "    n = x.shape[1]\n",
    "    first_factor = -20 * torch.exp(-0.2 * torch.sqrt((x**2).sum(dim=1) / n))\n",
    "    second_factor = -torch.exp((torch.cos(2 * math.pi * x)).sum(dim=1) / n)\n",
    "    return first_factor + second_factor + 20 + math.e\n",
    "\n",
    "\n",
    "def griewank(x: Tensor) -> Tensor:\n",
    "    \"\"\"Computes the energy of the Griewank function.\"\"\"\n",
    "    n = x.shape[1]\n",
    "    first_factor = (x**2).sum(dim=1) / 4000\n",
    "    second_factor = torch.prod(\n",
    "        torch.cos(x / torch.sqrt(torch.arange(1, n + 1, device=x.device))), dim=1\n",
    "    )\n",
    "    return first_factor - second_factor + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanShift(nn.Module):\n",
    "    \"\"\"Gaussian Mean-Shift clustering.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sigma: float,\n",
    "        damping: float = 0.5,\n",
    "        max_iter: int = 50,\n",
    "        shift_tol: float = 1e-4,\n",
    "        alpha: float = 2,\n",
    "        *,\n",
    "        learnable_sigma: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the clustering algorithm.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.sigma = nn.Parameter(\n",
    "            torch.tensor(float(sigma)), requires_grad=learnable_sigma\n",
    "        )\n",
    "        self.damping = damping\n",
    "        self.max_iter = max_iter\n",
    "        self.shift_tol = shift_tol\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def find_centroids(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Finds the centroids of the clusters using the mean-shift algorithm.\n",
    "\n",
    "        Args:\n",
    "            x: The input data tensor. This is expected to be a 2D tensor of shape\n",
    "                (N, D), where N is the number of data points and D is the dimensionality\n",
    "                of the data.\n",
    "\n",
    "        Returns:\n",
    "            The centroids of the clusters. This is a 2D tensor of shape (C, D), where C\n",
    "            is the number of clusters.\n",
    "        \"\"\"\n",
    "        centroids = x.clone()\n",
    "        gamma = 1 / (2 * self.sigma**2)\n",
    "        for _ in range(self.max_iter):\n",
    "            dist = torch.cdist(x, centroids, p=2)  # (N, N)\n",
    "            weight = torch.exp(-(dist**2) * gamma)  # (N, N)\n",
    "\n",
    "            nominator = weight @ x  # (N, D)\n",
    "            denominator = weight.sum(dim=1, keepdim=True)  # (N, 1)\n",
    "            new_centroids = nominator / denominator\n",
    "\n",
    "            shift = (new_centroids - centroids).norm(dim=1, p=2)  # (N,)\n",
    "            if shift.max() < self.shift_tol:\n",
    "                break\n",
    "\n",
    "            centroids = self.damping * centroids + (1 - self.damping) * new_centroids\n",
    "\n",
    "        centroids = _nms(x, centroids, threshold=self.sigma.item())\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def assign(self, x: Tensor, centroids: Tensor) -> Tensor:\n",
    "        \"\"\"Assigns the data points to the clusters.\n",
    "\n",
    "        Args:\n",
    "            x: The input data tensor. This is expected to be a 2D tensor of shape\n",
    "                (N, D), where N is the number of data points and D is the dimensionality\n",
    "                of the data.\n",
    "            centroids: The centroids of the clusters. This is a 2D tensor of shape\n",
    "                (C, D), where C is the number of clusters.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (N, C), where each row is a probability distribution over\n",
    "            the clusters.\n",
    "        \"\"\"\n",
    "        cost = -torch.cdist(x, centroids, p=2)  # (N, C)\n",
    "\n",
    "        match self.alpha:\n",
    "            case 1:\n",
    "                assignment = torch.softmax(cost, dim=1)\n",
    "            case 1.5:\n",
    "                assignment = entmax.entmax15(cost, dim=1)\n",
    "            case 2:\n",
    "                assignment = entmax.sparsemax(cost, dim=1)\n",
    "            case math.inf:\n",
    "                assigned_to = cost.argmax(dim=1)  # (N,)\n",
    "                assignment = torch.zeros_like(cost)\n",
    "                assignment[torch.arange(x.size(0), device=x.device), assigned_to] = 1\n",
    "                # use straight-through estimator\n",
    "                assignment = assignment - cost.detach() + cost\n",
    "            case _:\n",
    "                msg = f\"Invalid alpha value: {self.alpha}.\"\n",
    "                raise RuntimeError(msg)\n",
    "\n",
    "        return assignment  # type: ignore\n",
    "\n",
    "    def __call__(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Performs mean-shift clustering on the input data.\n",
    "\n",
    "        Args:\n",
    "            x: The input data tensor. This is expected to be a 2D tensor of shape\n",
    "                (N, D), where N is the number of data points and D is the dimensionality\n",
    "                of the data.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the centroids and the assignment of the data points to\n",
    "            the clusters. The centroids tensor has shape (C, D), where C is the number\n",
    "            of clusters. The assignment tensor has shape (N, C), where each row is a\n",
    "            probability distribution over the clusters.\n",
    "        \"\"\"\n",
    "        centroids = self.find_centroids(x)\n",
    "        assignment = self.assign(x, centroids)\n",
    "\n",
    "        return centroids, assignment\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Helper Functions\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "def _nms(x: Tensor, centroids: Tensor, threshold: float) -> Tensor:\n",
    "    pc_dist = torch.cdist(x, centroids, p=2)  # (N, N)\n",
    "    _, clostest_centroid = pc_dist.min(dim=1)  # (N,)\n",
    "    uniques, counts = clostest_centroid.unique(return_counts=True)\n",
    "    scores = torch.zeros_like(clostest_centroid)\n",
    "    scores[uniques] = counts\n",
    "\n",
    "    cc_dist = torch.cdist(centroids, centroids, p=2)\n",
    "    overlap = cc_dist < threshold\n",
    "    overlap.fill_diagonal_(fill_value=False)\n",
    "\n",
    "    order = scores.argsort(descending=True)\n",
    "    keep = torch.ones_like(scores, dtype=torch.bool)\n",
    "    for idx in order:\n",
    "        if keep[idx]:\n",
    "            keep &= ~overlap[idx]\n",
    "            keep[idx] = scores[idx] > 0\n",
    "\n",
    "    return centroids[keep]  # (C, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneEmbeddings(nn.Module):\n",
    "    \"\"\"Embeds genes values into a high-dimensional space.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        degree: int = 3,\n",
    "        grid_size: int = 5,\n",
    "        embed_dim: int = 64,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.degree = 3\n",
    "        num_internal_knots = grid_size + 1\n",
    "        num_knots = num_internal_knots + 2 * degree\n",
    "        knots = torch.linspace(-1, 1, num_internal_knots)\n",
    "        diff = knots[1] - knots[0]\n",
    "        knots = torch.cat([\n",
    "            knots[0] - diff * torch.arange(degree, 0, -1),\n",
    "            knots,\n",
    "            knots[-1] + diff * torch.arange(1, degree + 1),\n",
    "        ])\n",
    "        self.register_buffer(\"knots\", knots)\n",
    "        self.knots: Tensor\n",
    "\n",
    "        num_control_points = num_knots - degree - 1\n",
    "        # n_splines = embed_dim\n",
    "        cp = torch.empty(embed_dim, num_control_points)\n",
    "        self.cp = nn.Parameter(cp, requires_grad=True)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Embeds the gene values into a high-dimensional space.\n",
    "\n",
    "        Args:\n",
    "            x: The gene values to embed. This should be a tensor of shape `(B, N)`,\n",
    "                where `B` is the batch size and `N` is the number of genes.\n",
    "\n",
    "        Returns:\n",
    "            The embedded gene values. This is a tensor of shape `(B, N, D)`, where `D`\n",
    "            is the embedding dimension.\n",
    "        \"\"\"\n",
    "        cp = self.cp  # (embed_dim, num_control_points)\n",
    "\n",
    "        B, N = x.shape\n",
    "        x = x.flatten()  # (B * N,)\n",
    "        x = x.unsqueeze(0).expand(cp.size(0), -1)  # (embed_dim, B * N)\n",
    "\n",
    "        knots = self.knots.unsqueeze(0).expand(cp.size(0), -1)  # (embed_dim, num_knots)\n",
    "\n",
    "        embeds = compute_b_spline(x, knots, cp, self.degree)  # (embed_dim, B * N)\n",
    "        embeds = embeds.view(cp.size(0), B, N)  # (embed_dim, B, N)\n",
    "        embeds = embeds.permute(1, 2, 0)  # (B, N, embed_dim)\n",
    "\n",
    "        return embeds\n",
    "\n",
    "    @torch.no_grad()  # type: ignore\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.normal_(self.cp, mean=0.0, std=0.1)\n",
    "\n",
    "\n",
    "def compute_basis_functions(x: Tensor, knots: Tensor, k: int) -> Tensor:\n",
    "    \"\"\"Computes the b-spline basis functions for the given input data.\n",
    "\n",
    "    Args:\n",
    "        x: The points at which to evaluate the basis functions. This should be a\n",
    "            tensor of shape `(n_splines, n_points)`.\n",
    "        knots: The knot vector. This should be a tensor of shape `(n_splines, n_knots)`.\n",
    "        k: The degree of the spline.\n",
    "\n",
    "    Returns:\n",
    "        The basis functions evaluated at the given points. This is a tensor of\n",
    "        shape `(n_splines, n_basis_functions, n_points)`. The number of basis functions\n",
    "        is equal to `n_knots - k - 1`.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        x = x.unsqueeze(1)  # (n_splines, 1, n_points)\n",
    "        knots = knots.unsqueeze(2)  # (n_splines, n_knots, 1)\n",
    "        bf_k = (x >= knots[:, :-1]) & (x < knots[:, 1:])\n",
    "        bf_k = bf_k.to(x.dtype)\n",
    "    else:\n",
    "        bf_k_minus_1 = compute_basis_functions(x, knots, k - 1)\n",
    "        x = x.unsqueeze(1)  # (n_splines, 1, n_points)\n",
    "        knots = knots.unsqueeze(2)  # (n_splines, n_knots, 1)\n",
    "\n",
    "        first = (x - knots[:, : -(k + 1)]) / (knots[:, k:-1] - knots[:, : -(k + 1)])\n",
    "        first = first * bf_k_minus_1[:, :-1]\n",
    "\n",
    "        second = (knots[:, k + 1 :] - x) / (knots[:, k + 1 :] - knots[:, 1:(-k)])\n",
    "        second = second * bf_k_minus_1[:, 1:]\n",
    "\n",
    "        bf_k = first + second\n",
    "\n",
    "    return bf_k\n",
    "\n",
    "\n",
    "def compute_b_spline(\n",
    "    x: Tensor,\n",
    "    knots: Tensor,\n",
    "    coeffs: Tensor,\n",
    "    k: int,\n",
    ") -> Tensor:\n",
    "    \"\"\"Computes the b-spline function for the given input data.\n",
    "\n",
    "    Args:\n",
    "        x: The points at which to evaluate the b-spline. This should be a tensor\n",
    "            of shape `(n_splines, n_points)`.\n",
    "        knots: The knot vector. This should be a tensor of shape `(n_splines, n_knots)`.\n",
    "        coeffs: The coefficients of the b-spline. This should be a tensor of shape\n",
    "            `(n_splines, n_coeffs)`.\n",
    "        k: The degree of the spline.\n",
    "\n",
    "    Returns:\n",
    "        The b-spline evaluated at the given points. This is a tensor of shape\n",
    "        `(n_splines, n_points)`.\n",
    "    \"\"\"\n",
    "    # (n_splines, n_coeffs, n_points)\n",
    "    basis_functions = compute_basis_functions(x, knots, k)\n",
    "    # (n_splines, n_points)\n",
    "    b_spline = torch.einsum(\"ij,ijk->ik\", coeffs, basis_functions)\n",
    "\n",
    "    return b_spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperGraphConvolution(nn.Module):\n",
    "    \"\"\"A hypergraph convolution module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_layers: int = 1,\n",
    "        activation: Callable[[], nn.Module] = nn.ReLU,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.ModuleList([\n",
    "            nn.Linear(embed_dim, embed_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.activation = activation()\n",
    "\n",
    "    def __call__(self, x: Tensor, b: Tensor) -> Tensor:\n",
    "        \"\"\"Performs a hypergraph convolution.\n",
    "\n",
    "        Args:\n",
    "            x: The nodes embeddings. This should be a tensor of shape `(B, N, D)`.\n",
    "                where `N` is the number of nodes and `D` is the dimensionality of the\n",
    "                embeddings.\n",
    "            b: The hypergraph incidence matrix. This should be a tensor of shape\n",
    "                `(N, M)`, where `M` is the number of hyperedges.\n",
    "\n",
    "        Returns:\n",
    "            The updated node embeddings. This is a tensor of shape `(B, N, D)`.\n",
    "        \"\"\"\n",
    "        adj = b @ b.T  # (N, N)\n",
    "        adj = adj.unsqueeze(0)  # (1, N, N)\n",
    "        for layer in self.proj:\n",
    "            x = layer(x)  # (B, N, D)\n",
    "            x = adj @ x  # (B, N, D)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDEN(nn.Module):\n",
    "    \"\"\"Estimation of Distribution using Energy-based models (EDEN).\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_genes: int,\n",
    "        embed_dim: int,\n",
    "        meanshift: MeanShift,\n",
    "        gene_embeds: GeneEmbeddings,\n",
    "        hconv: HyperGraphConvolution,\n",
    "        pooling: Literal[\"max\", \"mean\", \"sum\"] = \"max\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.memory = nn.Parameter(torch.randn(num_genes, embed_dim))\n",
    "\n",
    "        self.meanshift = meanshift\n",
    "        self.gene_embeds = gene_embeds\n",
    "        self.hconv = hconv\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return f\"spline-{self.pooling}\"\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Computes the energy of the data points.\n",
    "\n",
    "        Args:\n",
    "            x: The data points to compute the energy of. This should be a tensor of\n",
    "                shape `(B, N)`, where `B` is the batch size and `N` is the number of\n",
    "                genes.\n",
    "\n",
    "        Returns:\n",
    "            The energy of the data points. This is a tensor of shape `(B,)`.\n",
    "        \"\"\"\n",
    "        _, assignment = self.meanshift(self.memory)\n",
    "        embeds = self.gene_embeds(x)\n",
    "        embeds = self.hconv(embeds, assignment)  # (B, N, D)\n",
    "\n",
    "        match self.pooling:\n",
    "            case \"max\":\n",
    "                embeds = embeds.max(dim=1).values\n",
    "            case \"mean\":\n",
    "                embeds = embeds.mean(dim=1)\n",
    "            case \"sum\":\n",
    "                embeds = embeds.sum(dim=1)\n",
    "            case _:\n",
    "                msg = f\"Unknown pooling method: {self.pooling}\"\n",
    "                raise ValueError(msg)\n",
    "\n",
    "        energies = self.mlp(embeds)  # (B, 1)\n",
    "        return energies.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all hyperparameters\n",
    "# set these to the values you want\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "RANGE = (-5, 5)\n",
    "NUM_POINTS = 10000\n",
    "GENOME_SIZE = 10\n",
    "BASE_LR = 1e-3\n",
    "BASE_NOISE = 1e-4\n",
    "QUANTILE = 0.2\n",
    "NUM_EXTERNAL_EPOCHS = 10\n",
    "NUM_INTERNAL_EPOCHS = 10\n",
    "\n",
    "# training loop hyperparameters\n",
    "FITNESS_BOUND = 5**2 * GENOME_SIZE\n",
    "INITIAL_DATASET_SIZE = 1_000\n",
    "BATCH_DIM = 128\n",
    "TRAINING_SUBSET_SIZE = (10000 - INITIAL_DATASET_SIZE) // NUM_EXTERNAL_EPOCHS\n",
    "\n",
    "# oracle\n",
    "energy = sphere\n",
    "\n",
    "# energy-based model\n",
    "gene_embeddings = GeneEmbeddings(embed_dim=64)\n",
    "ms = MeanShift(sigma=0.1)\n",
    "hconv = HyperGraphConvolution(embed_dim=64, num_layers=1)\n",
    "model = EDEN(\n",
    "    num_genes=GENOME_SIZE,\n",
    "    embed_dim=64,\n",
    "    meanshift=ms,\n",
    "    gene_embeds=gene_embeddings,\n",
    "    hconv=hconv,\n",
    "    pooling=\"max\",\n",
    ")\n",
    "model = model.cuda()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "# we found schedulers to be detrimental to the training\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, NUM_EPOCHS, eta_min=1e-6)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=NUM_EPOCHS, gamma=0.1)\n",
    "\n",
    "\n",
    "def train(loader: data.DataLoader[tuple[Tensor, Tensor]]) -> None:\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "    for _ in tqdm(range(NUM_INTERNAL_EPOCHS), desc=\"Internal Training\", total=NUM_INTERNAL_EPOCHS):  # fmt: skip\n",
    "        optimizer.zero_grad()\n",
    "        for x, y in loader:\n",
    "            x, y = x.cuda(), y.cuda()  # noqa: PLW2901\n",
    "            e = model(x)\n",
    "            loss = F.mse_loss(e, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "def langevin(x: Tensor, step_size: Tensor, noise_size: Tensor | None = None) -> Tensor:\n",
    "    \"\"\"Samples from a distribution using Langevin dynamics.\"\"\"\n",
    "    x = x.clone()\n",
    "    x.requires_grad_(True)\n",
    "    if noise_size is None:\n",
    "        noise_size = step_size**2\n",
    "\n",
    "    noise = torch.empty_like(x, requires_grad=False)\n",
    "\n",
    "    for _ in range(50):\n",
    "        y = energy(x).sum()\n",
    "        grad = torch.autograd.grad(y, x)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x.sub_(step_size * grad)\n",
    "            x.clamp_(*RANGE)\n",
    "\n",
    "            noise.normal_(0, 1)\n",
    "            x.add_(noise.mul_(noise_size))\n",
    "            x.clamp_(*RANGE)\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "\n",
    "def loop() -> Tensor:  # noqa: PLR0915\n",
    "    \"\"\"Main optimization loop.\"\"\"\n",
    "    # populate buffer with initial samples\n",
    "    x = torch.rand(INITIAL_DATASET_SIZE, GENOME_SIZE).mul_(2).sub_(1)\n",
    "    y = energy(change_range(x, (-1, 1), (-5, 5)) ** 2) / FITNESS_BOUND\n",
    "\n",
    "    dataset = data.TensorDataset(x, y)\n",
    "    loader = data.DataLoader(dataset, batch_size=BATCH_DIM, shuffle=True)\n",
    "\n",
    "    x = torch.rand((NUM_POINTS, GENOME_SIZE), device=\"cuda\").mul_(2).sub_(1)\n",
    "    lr = torch.full_like(x, BASE_LR)\n",
    "    noise_size = torch.full_like(x, BASE_NOISE)\n",
    "    prev_clusters = []\n",
    "\n",
    "    train(loader)  # type: ignore\n",
    "\n",
    "    for _ in tqdm(range(NUM_EXTERNAL_EPOCHS), desc=\"Optimization\", total=NUM_EXTERNAL_EPOCHS):  # fmt: skip\n",
    "        x = langevin(x, lr, noise_size)\n",
    "\n",
    "        bandwidth = estimate_bandwidth(x, quantile=QUANTILE)\n",
    "        centroids, labels = meanshift(x, num_iters=100, bandwidth=bandwidth)\n",
    "\n",
    "        new_clusters: list[tuple[Tensor, float]] = []\n",
    "        for j in range(centroids.shape[0]):\n",
    "            points = x[labels == j]\n",
    "            fitness = float(energy(points).mean())\n",
    "            new_clusters.append((points, fitness))\n",
    "\n",
    "        f_denominator = sum(1 / f for _, f in new_clusters)\n",
    "\n",
    "        new_points = []\n",
    "        new_noise_sizes = []\n",
    "        for i, (c_points, c_fitness) in enumerate(new_clusters):\n",
    "            B = math.ceil(NUM_POINTS * (1 / c_fitness) / f_denominator)\n",
    "            if B <= GENOME_SIZE:\n",
    "                continue\n",
    "\n",
    "            mean, cov = torch.mean(c_points, dim=0), torch.cov(c_points.T)\n",
    "            # make sure the covariance matrix is positive definite\n",
    "            cov = cov + torch.eye(GENOME_SIZE, device=cov.device) * 1e-6\n",
    "            mn = torch.distributions.MultivariateNormal(mean, cov)\n",
    "            p = mn.rsample((B,))  # type: ignore\n",
    "            new_points.append(p)\n",
    "\n",
    "            best_match, best_icd = None, torch.inf\n",
    "            for j in range(len(prev_clusters)):\n",
    "                icd = distance_matrix(c_points, prev_clusters[j][0])\n",
    "                icd = icd.min(dim=1).values.mean().item()\n",
    "                # print(f\"ICD: {icd:.2f}, bandwidth: {bandwidth:.2f}\")\n",
    "                if icd < best_icd and icd < bandwidth:\n",
    "                    best_match, best_icd = j, icd\n",
    "\n",
    "            if best_match is not None:\n",
    "                # print(f\"Matched with cluster {best_match}\")\n",
    "                _, prev_fitness = prev_clusters[best_match]\n",
    "                factor = math.sqrt(c_fitness / prev_fitness)\n",
    "                new_clusters[i] = (c_points, min(c_fitness, prev_fitness))\n",
    "                # print(f\"Factor: {factor:.2f}\")\n",
    "            else:\n",
    "                factor = 1.0\n",
    "\n",
    "            extension = c_points.max(dim=0).values - c_points.min(dim=0).values / 2\n",
    "            c_noise_size = torch.full((B, GENOME_SIZE), BASE_NOISE, device=\"cuda\")\n",
    "            c_noise_size = c_noise_size * extension * factor\n",
    "            new_noise_sizes.append(c_noise_size)\n",
    "\n",
    "        prev_clusters = new_clusters\n",
    "        x = torch.cat(new_points, dim=0)\n",
    "        x.clamp_(*RANGE)\n",
    "        lr = torch.full_like(x, BASE_LR)\n",
    "        noise_size = torch.cat(new_noise_sizes, dim=0)\n",
    "\n",
    "        # add new points to the dataset\n",
    "        indices = torch.randint(0, x.shape[0], (TRAINING_SUBSET_SIZE,), device=\"cuda\")\n",
    "        x_subset = x[indices]\n",
    "        y_subset = energy(change_range(x_subset, (-1, 1), (-5, 5)) ** 2) / FITNESS_BOUND\n",
    "\n",
    "        old_x, old_y = dataset.tensors\n",
    "        new_x = torch.cat([old_x, x_subset.cpu()], dim=0)\n",
    "        new_y = torch.cat([old_y, y_subset.cpu()], dim=0)\n",
    "        dataset = data.TensorDataset(new_x, new_y)\n",
    "        loader = data.DataLoader(dataset, batch_size=BATCH_DIM, shuffle=True)\n",
    "        train(loader)  # type: ignore\n",
    "\n",
    "    # sample from the found points the best one\n",
    "    e = model(x)\n",
    "    idx, _ = e.min(dim=0)\n",
    "    return x[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splot the 2d landscape\n",
    "# let's  not focus on the whole space, but on the [-r, r] x [-r, r] subspace\n",
    "# to verify whether the model has correctly learned the landscape for the points\n",
    "# with better fitness\n",
    "NUM_SAMPLES = 200\n",
    "with torch.no_grad():\n",
    "    low, high = -0.001, 0.001\n",
    "    elev, azim = None, None\n",
    "    x = torch.linspace(low, high, NUM_SAMPLES)\n",
    "    y = torch.linspace(low, high, NUM_SAMPLES)\n",
    "    points = torch.cartesian_prod(x, y)\n",
    "    X = points[:, 0].reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "    Y = points[:, 1].reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "    # X, Y = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "    Z = X**2 + Y**2\n",
    "\n",
    "    n_plots = len(models) + 1\n",
    "    fig = plt.figure(figsize=(n_plots * 5, 5))\n",
    "    axes = fig.subplots(1, n_plots, subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "    axes[0].plot_surface(X.cpu(), Y.cpu(), Z.cpu(), cmap=\"viridis\")\n",
    "    axes[0].set_title(\"Ground truth\")\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_zlabel(\"fitness\")\n",
    "    axes[0].view_init(elev=elev, azim=azim)\n",
    "\n",
    "    solutions = points.cuda()\n",
    "    solutions = change_range(solutions, (-5, 5), (-1, 1))\n",
    "\n",
    "    for j, model in enumerate(models):\n",
    "        f_hat = model(solutions) * FITNESS_BOUND\n",
    "        f_hat = f_hat.reshape(NUM_SAMPLES, NUM_SAMPLES)\n",
    "\n",
    "        axes[j + 1].plot_surface(X.cpu(), Y.cpu(), f_hat.cpu(), cmap=\"viridis\")\n",
    "        axes[j + 1].set_title(model.name)\n",
    "        axes[j + 1].set_xlabel(\"x\")\n",
    "        axes[j + 1].set_ylabel(\"y\")\n",
    "        axes[j + 1].set_zlabel(\"fitness\")\n",
    "\n",
    "        # change rotation\n",
    "        axes[j + 1].view_init(elev=elev, azim=azim)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
